# ğŸ Run LLM Locally with Ollama

This guide walks you through setting up and running LLM model's on your local machine using **[Ollama](https://ollama.com)** â€” a lightweight framework to run large language models effortlessly.

## ğŸ› ï¸ Step 1: Install Ollama

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

â„¹ï¸ You can also download installers manually: https://ollama.com/download

## ğŸ“¦ Step 2: Pull the Model of your choice

```bash
ollama pull llama3
```

_Note_ This will download the llama3 8B model to your local machine.

## â–¶ï¸ Step 3: Run the Model

```bash
ollama run llama3
```

This starts a conversational session with the model in your terminal.

## ğŸ§¼ List All Models

```bash
ollama list
```

## ğŸ§¼ Remove models

```bash
ollama rm llama2:latest
```
