{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f460ef64-4c59-457c-8409-c740ffbcf489",
   "metadata": {},
   "source": [
    "# üîç Hello World with LLMs: Personalized Career Advice\n",
    "\n",
    "Compare outputs from popular LLMs by asking each one to suggest 3 career paths based on a user profile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1c17c0-4f73-4eb0-b955-8d9a65a997a5",
   "metadata": {},
   "source": [
    "## üìÇ Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f912d0-78eb-41b7-b129-ce377b472d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install openai anthropic google-generativeai cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d201b1-27c5-4aba-bc24-d0a58b276723",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables stored in a file called .env\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133c7057-7152-4634-8f00-56becea6afbb",
   "metadata": {},
   "source": [
    "## üë§ Define the User Profile\n",
    "\n",
    "Instead of hardcoding the same values into the prompt multiple times, we use a Python dictionary to define the user details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bbc58b-c7ba-450f-b3fb-0f959f1df2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_profile = {\n",
    "    \"name\": \"Prasath Basuvaraj\",\n",
    "    \"current_role\": \"Software Architect\",\n",
    "    \"experience_years\": 20,\n",
    "    \"skills\": [\"Java\", \"Spring Boot\", \"React\", \"AWS\", \"Microservices\"],\n",
    "    \"goal\": \"career growth and stability\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a78fe1-60d3-4aac-9776-d4a83ca0290f",
   "metadata": {},
   "source": [
    "## ‚úçÔ∏è Construct the Prompt\n",
    "\n",
    "We then dynamically build a prompt using this profile. This way, every LLM receives the same input in a structured and natural language format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095a8bce-74d8-47c7-a76a-075f7f5159f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = (\n",
    "    f\"Given the following user profile:\\\\n\"\n",
    "    f\"Name: {user_profile['name']}\\\\n\"\n",
    "    f\"Current Role: {user_profile['current_role']}\\\\n\"\n",
    "    f\"Experience: {user_profile['experience_years']} years\\\\n\"\n",
    "    f\"Skills: {', '.join(user_profile['skills'])}\\\\n\"\n",
    "    f\"Goal: {user_profile['goal']}\\\\n\"\n",
    "    \"Suggest 3 career paths this person should consider in 2025 and explain why.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e6a599-87dc-4fbf-88ee-098276fd4aaf",
   "metadata": {},
   "source": [
    "This prompt is then passed to each LLM (cloud-based or local) so we can compare how each model interprets and responds to the same request."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffba2812-4f22-4513-a574-785b3419ab24",
   "metadata": {},
   "source": [
    "## üî∑ OpenAI GPT-4\n",
    "\n",
    "This section does two things:\n",
    "\n",
    "**Validates your OpenAI API key**\n",
    "Before making any API calls, we check that the key is properly set via environment variables and follows the expected format (sk-proj-...). This helps avoid silent failures or confusing authentication errors.\n",
    "\n",
    "**Sends the prompt to GPT-4**\n",
    "Using the openai Python SDK, we send a structured prompt to the GPT-4 model. The prompt includes the user profile and a request for career path suggestions. The model responds as if it were a helpful career advisor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df9b198-6bb7-49f9-80e1-d79f25fed5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Check the OpenAI Api key\n",
    "if not openai_api_key:\n",
    "    print(\"No API key was found - please head over to the troubleshooting notebook in this folder to identify & fix!\")\n",
    "elif not openai_api_key.startswith(\"sk-proj-\"):\n",
    "    print(\"An API key was found, but it doesn't start sk-proj-; please check you're using the right key - see troubleshooting notebook\")\n",
    "elif openai_api_key.strip() != openai_api_key:\n",
    "    print(\"An API key was found, but it looks like it might have space or tab characters at the start or end - please remove them - see troubleshooting notebook\")\n",
    "else:\n",
    "    print(\"API key found and looks good!\")\n",
    "\n",
    "\n",
    "def query_openai(prompt):\n",
    "    client = openai.OpenAI(api_key=openai_api_key)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful career advisor.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Run and display response\n",
    "openai_response = query_openai(prompt_template)\n",
    "print(\"üî∑ OpenAI GPT-4 Response:\")\n",
    "print(openai_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11422d8c-5840-4f36-aa6d-c6a602351c7f",
   "metadata": {},
   "source": [
    "_üìå Note_: We'll follow a similar approach for the other cloud-based models‚Ää-‚ÄäClaude, Gemini, and Cohere."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b2981c-7e08-409a-bfac-dbb36617531a",
   "metadata": {},
   "source": [
    "## üü£ Claude 3 (Anthropic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6098dd-58c6-49a0-8ea6-c96f25e1c54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "\n",
    "claude_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "# Check the Claude API key\n",
    "if not claude_api_key:\n",
    "    print(\"No Anthropic (Claude) API key was found ‚Äî please set your ANTHROPIC_API_KEY environment variable.\")\n",
    "elif not claude_api_key.startswith(\"sk-ant-\"):\n",
    "    print(\"An API key was found, but it doesn't start with 'sk-ant-'; please check that you're using the correct Claude key format.\")\n",
    "elif claude_api_key.strip() != claude_api_key:\n",
    "    print(\"An API key was found, but it appears to have leading/trailing whitespace. Please remove them.\")\n",
    "else:\n",
    "    print(\"Claude API key found and looks good!\")\n",
    "\n",
    "\n",
    "def query_claude(prompt):\n",
    "    client = anthropic.Anthropic(api_key=claude_api_key)\n",
    "    response = client.messages.create(\n",
    "        model=\"claude-sonnet-4-20250514\",\n",
    "        max_tokens=1000,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.content[0].text\n",
    "\n",
    "claude_response = query_claude(prompt_template)\n",
    "print(\"üü£ Claude 3 Response:\")\n",
    "print(claude_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e445ce3c-459e-4591-bba0-a26b2dedf96c",
   "metadata": {},
   "source": [
    "## üü° Gemini 1.5 Pro (Google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbfd986-3d38-4b1b-bf36-955429bd7ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "\n",
    "gemini_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Check the Gemini API key\n",
    "if not gemini_api_key:\n",
    "    print(\"No Gemini (Google) API key was found ‚Äî please set your GOOGLE_API_KEY environment variable.\")\n",
    "elif not gemini_api_key.startswith(\"AIza\"):\n",
    "    print(\"An API key was found, but it doesn't start with 'AIza'; please check that you're using a valid Google API key.\")\n",
    "elif gemini_api_key.strip() != gemini_api_key:\n",
    "    print(\"An API key was found, but it appears to have leading/trailing whitespace. Please remove them.\")\n",
    "else:\n",
    "    print(\"Gemini API key found and looks good!\")\n",
    "\n",
    "# Configure Gemini\n",
    "genai.configure(api_key=gemini_api_key)    \n",
    "\n",
    "def query_gemini(prompt):\n",
    "    model = genai.GenerativeModel(\"gemini-1.5-pro-latest\")\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "# Run and display response\n",
    "gemini_response = query_gemini(prompt_template)\n",
    "print(\"üü° Gemini Response:\")\n",
    "print(gemini_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e7b837-bb03-42fd-be18-42dafdb228c7",
   "metadata": {},
   "source": [
    "## üü¢ Cohere Command R+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeab7aa7-3ab2-4ad4-b4db-6881fcbb18bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "\n",
    "cohere_api_key = os.getenv(\"COHERE_API_KEY\")\n",
    "\n",
    "# Check the Cohere API key\n",
    "if not cohere_api_key:\n",
    "    print(\"No Cohere API key was found ‚Äî please set your COHERE_API_KEY environment variable.\")\n",
    "elif not cohere_api_key.startswith(\"sk-\"):\n",
    "    print(\"An API key was found, but it doesn't start with 'sk-'; please check that you're using a valid Cohere key.\")\n",
    "elif cohere_api_key.strip() != cohere_api_key:\n",
    "    print(\"An API key was found, but it appears to have leading/trailing whitespace. Please remove them.\")\n",
    "else:\n",
    "    print(\"Cohere API key found and looks good!\")\n",
    "\n",
    "cohere_client = cohere.Client(api_key=cohere_api_key)\n",
    "\n",
    "def query_cohere(prompt):\n",
    "    response = cohere_client.chat(\n",
    "        model=\"command-r-plus\",\n",
    "        message=prompt,\n",
    "        temperature=0.7,\n",
    "        max_tokens=1000\n",
    "    )\n",
    "    return response.text\n",
    "\n",
    "cohere_response = query_cohere(prompt_template)\n",
    "print(\"üü¢ Cohere Response:\")\n",
    "print(cohere_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
